{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emmanuel Decena - ME EE**\n",
    "\n",
    "**Foundations of Machine Learning**\n",
    "\n",
    "Assignment 2 - Numpy Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask the user to supply the mean and standard deviation of a 1D Gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the given mean and std, draw 1000 random samples from ‚àí2ùúé, +2ùúé to build a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.normal(mu, 2*sigma, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbin = 30\n",
    "count, bins, ignored = plt.hist(s, nbin, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into train (90%) and test(10%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
    "y = np.asarray(count)[..., np.newaxis]\n",
    "X = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X)\n",
    "slicesplit = int(0.9*nbin)\n",
    "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training[0:1:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in training:\n",
    "#    print(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
    "- Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
    "- Initialize the weights using a Gaussian distribution with zero mean and std=1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "    # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "    \n",
    "    \n",
    "class Loss:\n",
    "    # Calculates the data losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    # Mean Squared Error loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate = 0.1): \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Sigmoid() \n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for 20 epochs and evaluate the performance of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.5993036014519144, Test loss 0.5183873194652288\n",
      "Epoch: 1, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 2, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 3, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 4, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 5, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 6, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 7, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 8, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 9, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 10, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 11, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 12, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 13, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 14, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 15, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 16, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 17, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 18, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n",
      "Epoch: 19, Train loss: 0.5337063075052776, Test loss 0.5183873194652288\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Training\n",
    "    avetrainloss = []\n",
    "    for row in training: #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        trainloss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "        # Backward pass \n",
    "        loss_function.backward(activation3.output, y)\n",
    "        \n",
    "        activation3.backward(loss_function.dinputs) \n",
    "        dense3.backward(activation3.dinputs) \n",
    "        \n",
    "        activation2.backward(dense3.dinputs) \n",
    "        dense2.backward(activation2.dinputs) \n",
    "        \n",
    "        activation1.backward(dense2.dinputs) \n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1) \n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        \n",
    "        avetrainloss.append(trainloss)\n",
    "    \n",
    "    # Validation\n",
    "    avetestloss = []\n",
    "    for row in test:\n",
    "        #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        testloss = loss_function.calculate(activation3.output, y)\n",
    "        \n",
    "        avetestloss.append(testloss)\n",
    "        \n",
    "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-jupyterbook",
   "language": "python",
   "name": "env-jupyterbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
