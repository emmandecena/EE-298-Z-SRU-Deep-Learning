{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Emmanuel Decena - ME EE**\n",
    "\n",
    "**Foundations of Machine Learning**\n",
    "\n",
    "Assignment 2 - Numpy Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask the user to supply the mean and standard deviation of a 1D Gaussian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 0, 0.1 # mean and standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the given mean and std, draw  1,000,000 random samples from ‚àí2ùúé, +2ùúé to build a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.random.normal(mu, sigma, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbin = 1000\n",
    "count, bins, ignored = plt.hist(s, nbin, density=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the dataset into train (90%) and test(10%) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.asarray(bins[:-1])[..., np.newaxis]\n",
    "y = np.asarray(count)[..., np.newaxis]\n",
    "X = np.concatenate((x, y), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(X)\n",
    "slicesplit = int(0.9*nbin)\n",
    "training, test = X[:slicesplit,:], X[slicesplit:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training[0:1:,0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for row in training:\n",
    "#    print(row[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pure Python3 and Numpy, build a 3-layer neural networks:\n",
    "- Layers: {1 ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí ùëÖùëíùêøùëà ‚àí 64 ‚àí 1 ‚àí ùë†ùëñùëîùëöùëúùëñùëë} \n",
    "- Initialize the weights using a Gaussian distribution with zero mean and std=0.01.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "    # Derivative - calculates from output of the sigmoid function \n",
    "        self.dinputs = dvalues*(1 - self.output)*self.output\n",
    "    \n",
    "    \n",
    "class Loss:\n",
    "    # Calculates the data losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return loss \n",
    "        return data_loss\n",
    "    \n",
    "\n",
    "class Loss_MeanSquaredError(Loss): \n",
    "    # Mean Squared Error loss\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true): # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred)**2, axis=-1) \n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "    \n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them \n",
    "        outputs = len(dvalues[0])\n",
    "        \n",
    "        # Gradient on values\n",
    "        self.dinputs = -2*(y_true - dvalues)/outputs \n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "        \n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate = 0.1): \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = Layer_Dense(1, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "dense2 = Layer_Dense(64, 64)\n",
    "activation2 = Activation_ReLU()\n",
    "\n",
    "dense3 = Layer_Dense(64, 1)\n",
    "activation3 = Activation_Sigmoid() \n",
    "\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_SGD(learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train for 20 epochs and evaluate the performance of your network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 1.8127474211046868, Test loss 1.8748551102611482\n",
      "Epoch: 1, Train loss: 1.7934894698218609, Test loss 1.8726064707379537\n",
      "Epoch: 2, Train loss: 1.7923380159637623, Test loss 1.871963281964371\n",
      "Epoch: 3, Train loss: 1.7919406074083417, Test loss 1.8716679923608488\n",
      "Epoch: 4, Train loss: 1.7917478899328843, Test loss 1.8715007961287378\n",
      "Epoch: 5, Train loss: 1.7916373395514742, Test loss 1.871393803852848\n",
      "Epoch: 6, Train loss: 1.7915668198280819, Test loss 1.8713196046664624\n",
      "Epoch: 7, Train loss: 1.79151832580054, Test loss 1.8712648571194987\n",
      "Epoch: 8, Train loss: 1.791482837692734, Test loss 1.8712223740661806\n",
      "Epoch: 9, Train loss: 1.7914553473555452, Test loss 1.8711878004759297\n",
      "Epoch: 10, Train loss: 1.7914327710827405, Test loss 1.871158269073368\n",
      "Epoch: 11, Train loss: 1.791412956178075, Test loss 1.871131592699183\n",
      "Epoch: 12, Train loss: 1.7913942289181068, Test loss 1.8711059674214554\n",
      "Epoch: 13, Train loss: 1.791374811012174, Test loss 1.8710792690762474\n",
      "Epoch: 14, Train loss: 1.7913528683319624, Test loss 1.8710491415945476\n",
      "Epoch: 15, Train loss: 1.7913254588114214, Test loss 1.8710118167906489\n",
      "Epoch: 16, Train loss: 1.7912876446015162, Test loss 1.8709603607174816\n",
      "Epoch: 17, Train loss: 1.7912300076031422, Test loss 1.8708815182318392\n",
      "Epoch: 18, Train loss: 1.7911324462688767, Test loss 1.8707441443549453\n",
      "Epoch: 19, Train loss: 1.790941531406202, Test loss 1.8704674315664824\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS): \n",
    "    \n",
    "    # Training\n",
    "    avetrainloss = []\n",
    "    for row in training: #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        trainloss = loss_function.calculate(activation3.output, y)\n",
    "\n",
    "        # Backward pass \n",
    "        loss_function.backward(activation3.output, y)\n",
    "        \n",
    "        activation3.backward(loss_function.dinputs) \n",
    "        dense3.backward(activation3.dinputs) \n",
    "        \n",
    "        activation2.backward(dense3.dinputs) \n",
    "        dense2.backward(activation2.dinputs) \n",
    "        \n",
    "        activation1.backward(dense2.dinputs) \n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights and biases\n",
    "        optimizer.update_params(dense1) \n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        \n",
    "        avetrainloss.append(trainloss)\n",
    "    \n",
    "    # Validation\n",
    "    avetestloss = []\n",
    "    for row in test:\n",
    "        #Batch size = 1\n",
    "        X = row[0]\n",
    "        y = row[1]\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        \n",
    "        # Loss\n",
    "        testloss = loss_function.calculate(activation3.output, y)\n",
    "        \n",
    "        avetestloss.append(testloss)\n",
    "        \n",
    "    print(\"Epoch: {}, Train loss: {}, Test loss {}\".format(epoch,np.mean(avetrainloss),np.mean(avetestloss))) \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-jupyterbook",
   "language": "python",
   "name": "env-jupyterbook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
